#!/bin/bash

echo "ğŸ¦  Running init_sporeflow.sh"

while [[ $CONDA_DEFAULT_ENV ]]; do
    echo "ğŸ¦  Deactivating current Conda environment: $CONDA_DEFAULT_ENV"
    conda deactivate
done

if { conda env list | grep 'sporeflow-main'; } >/dev/null 2>&1; then
  echo "ğŸ¦  Environment sporeflow-main is already installed."
else
  echo "ğŸ¦  Installing environment sporeflow-main...."
  conda env create --file config/smk_conda.yml
  echo "ğŸ¦  Environment sporeflow-main installed successfully."
fi

echo "ğŸ¦  Activating environment..."
conda activate sporeflow-main

echo "ğŸ¦  Setting aliases..."

## Default njobs is 30. Feel free to change it in sf_run and sf_inmediate

alias sf_run='snakemake --use-conda --cluster "sbatch -J {cluster.jobname} -t {cluster.time} --mem {cluster.memory} -c {cluster.ncpus} -o {cluster.output} -e {cluster.error}" --cluster-config config/cluster_config.yml --jobs 30'
alias sf_immediate='snakemake --use-conda --cluster-config config/cluster_config.yml --jobs 30 -pr --immediate-submit --notemp --cluster "python3 workflow/scripts/immediate_submit.py {dependencies}"'
alias sf_draw_dag='snakemake -f --dag | dot -Tpdf > dag.pdf'
alias sf_draw_rulegraph='snakemake -f --rulegraph | dot -Tpdf > rulegraph.pdf'
alias sf_draw_filegraph='snakemake -f --filegraph | dot -Tpdf > filegraph.pdf'


echo "ğŸ¦  SporeFlow loaded successfully!"
echo "    ğŸ„ Use sf_run to run SporeFlow in a Slurm HPC queue system"
echo "    ğŸ„ Use sf_immediate to run SporeFlow and send all jobs to the queue system at once (not recommended)"
echo "    ğŸ„ Use sf_draw_dag to draw a DAG of the workflow in dag.pdf"
echo "    ğŸ„ Use sf_draw_rulegraph to draw a rule graph of the workflow in rulegraph.pdf"
echo "    ğŸ„ Use sf_draw_filegraph to draw a file graph of the workflow in filegraph.pdf"


